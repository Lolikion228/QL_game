Using Q-Learning for playing grid-based game
 =======================
 
 В рамках данного проекта была реализована простая grid-based игра и интерфейс для использования алгоритма Q-обучения c целью её прохождения.\
 Вышеупомянутый интерфейс представлен классом **Env()**, у которого есть методы **step()** и **reset()**. \
 Также был проведён анализ зависимости результатов обучения от параметров \
***gamma***, ***lr***, ***eps***, ***step_reward*** (см. **analysis.ipynb**)


Файлы проекта
=============


./main.py
-----------
В **main.py** перебирается некое множество комбинаций гиперпараметров и для каждой комбинации запускается алгоритм, после этого результат работы алгоритма пишется в **./logs/log.txt**

./game.py
-----------
Здесь находится реализация **Env()** и всех вспомогательных функций.\
при запуске данного файла через\
`python ./game.py eps gamma lr step_reward`\
произойдёт следующее:\
Сначала будет проинициализировано игровое поле размера ***w*** на ***h*** клеток,
на котором будет ***n_traps*** ловушек и ***n_energy*** энергетиков. После этого будут сгенерированы координаты каждой ловушки и каждого энергетика (они берутся из дискретного равномерного распределения) (поменять вид карты можно поменяв аргумент в np.random.seed()). Далее будет проинициализирована нулями ***q_table***.
После этого будет запущена функция **run\_experiment()** и внутри неё будет ***episodes_cnt*** раз запущена функция **run\_episode()**, и **run\_experiment()** вернёт статистику по эпизодам. После этого из всех эпизодов выбирается эпизод с набиольшей наградой и в консоль выводится статистика по этому эпизоду и время затраченное на работу функции **run\_experiment()**.\
\
*Объяснение смысла параметров ***eps***, ***gamma***, ***lr***, ***step_reward*** см. в **analysis.ipynb**



./logs/log.txt
---------------
Каждая строка лога имеет формат:\
***eps***   ***gamma***  ***lr***  ***step_reward***  ***reward***  ***moves***  ***traps*** ***energy***  ***time*** \
\
***reward*** - суммарная награда за эпизод\
***moves*** - количество ходов внутри эпизода\
***traps*** - количество активированных ловушек внутри эпизода\
***energy*** - количество собранных энергетиков внутри эпизода\
***time*** - время, затраченное на все эпизоды


./analysis.py
-----------
Здесь происходит анализ зависимости результатов обучения от параметров алгоритма


